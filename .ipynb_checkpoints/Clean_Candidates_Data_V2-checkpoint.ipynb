{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filename: Clean_Candidates_Data.ipynb\n",
    "#Description: Load in candidates.csv and set up for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'mostRecentStatus.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8659bdfe1ee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Candidate Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#candidates = pd.read_csv(\"candidates.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mostRecentStatus.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\python\\conda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\conda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\conda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\conda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\conda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'mostRecentStatus.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "#Candidate Data\n",
    "#candidates = pd.read_csv(\"candidates.csv\")\n",
    "mrs = pd.read_csv(\"mostRecentStatus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop variables that don't appear relevant\n",
    "# \"middleName\": mostly null\n",
    "# \"nickName\": mostly null\n",
    "# \"linkedInURL\": mostly null\n",
    "# \"address\": won't be using candidate address\n",
    "# \"sourceNotes: mostly null\n",
    "'''\n",
    "toDrop = ['middleName', 'nickName', 'linkedInUrl', 'address', 'sourceNotes']\n",
    "candidates = candidates.drop(toDrop, axis = 1)\n",
    "'''\n",
    "#Preprocessing drop\n",
    "#Merge address and address_field\n",
    "toDropMRS = ['nname', 'altemail', 'phone', 'job_id', 'joborder_status', 'status_html', 'fullname', 'recruiter_name',\n",
    "             'accman_name', 'name', 'source_notes', 'i9notes', 'h1b_entry_date', 'star', 'looking_status',\n",
    "             'recruiter_id2', 'recruiter_id3','source_id.1', 'status_name.1', 'address', 'address_field']\n",
    "for col in toDropMRS:\n",
    "    if(col in mrs.columns):\n",
    "        mrs = mrs.drop(col,1)\n",
    "\n",
    "mrs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean email\n",
    "#To Ryan: Should we try to extract the domain as an potential features? Are \"gmail\" users different \"aol\"\n",
    "\n",
    "#To Do\n",
    "mrs[\"is_gmail\"] = (1*(mrs[\"email\"].str.contains(\"@gmail\", na=False)))\n",
    "mrs[\"is_aol\"] = (1*(mrs[\"email\"].str.contains(\"@aol\", na=False)))\n",
    "mrs[\"is_custom\"] = (1*(~mrs[\"email\"].str.contains(\"@gmail|@aol\", na=False)))\n",
    "print(mrs[\"is_gmail\"].value_counts())\n",
    "print(mrs[\"is_aol\"].value_counts())\n",
    "print(mrs[\"is_custom\"].value_counts())\n",
    "\n",
    "\n",
    "#drop email in group later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean gender\n",
    "#To Ryan: Looks like all the candidates are men...is this the full sample?\n",
    "mrs[\"is_male\"] = (1*(mrs[\"gender\"] == \"Male\"))\n",
    "mrs.is_male.value_counts()\n",
    "\n",
    "#drop gender in group later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean isMinority\n",
    "#To Ryan: What do these values mean again?\n",
    "mrs[\"is_minority\"] = (1*(mrs[\"minority\"] == \"on\"))\n",
    "mrs.is_minority.value_counts()\n",
    "\n",
    "#drop minority in group later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean isIvy\n",
    "#To Ryan: What do these values mean again?\n",
    "mrs[\"is_ivy\"] = (1*(mrs[\"ivy\"] == \"on\"))\n",
    "mrs.is_ivy.value_counts()\n",
    "\n",
    "#drop ivy in group later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean recruiterId\n",
    "#Nothing more to do\n",
    "\n",
    "#candidates.recruiterId.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean candidateCreationDate\n",
    "#Nothing here to do\n",
    "\n",
    "#print(mrs.candidateCreationDate[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean zip\n",
    "#Nothing to do here?\n",
    "\n",
    "#To Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean isWillingRelocate\n",
    "#To Ryan: What do the \"-1\" mean?\n",
    "#mrs[\"is_ivy\"] = (1*(mrs[\"ivy\"] == \"on\"))\n",
    "#mrs.is_ivy.value_counts()\n",
    "mrs.willingToRelocate.value_counts()\n",
    "#candidates.isWillingRelocate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean sourceId\n",
    "#What is this feature?\n",
    "#Source where candidate originated from. Look in data for source names\n",
    "\n",
    "mrs.source_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean yearBeganExperience\n",
    "#Use the difference between when the candidate was inputted into the database and \"yearBeganExperience\"\n",
    "#Created \"yearsOfExp\"C\n",
    "nowYear = datetime.datetime.now().year\n",
    "mrs['yearsOfExp'] = nowYear-mrs['year_began']\n",
    "\n",
    "#drop year_began later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean i9\n",
    "#To Ryan: Should create dummies from this? Does USC-GC mean US Citizen?\n",
    "mrs[\"is_citizen\"] = (1*(mrs[\"i9\"].str.contains(\"Eligible|U.S. Citizen\", na=False)))\n",
    "\n",
    "print(mrs.i9.value_counts())\n",
    "mrs.is_citizen.value_counts()\n",
    "\n",
    "#drop i9 later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to clean the bottom features\n",
    "\n",
    "#Create Dummies for Functional Category\n",
    "#Also the total functional categories\n",
    "\n",
    "mrs[['func1', 'func2', 'func3', 'func4', 'func5', 'func6', 'func7']] = mrs['functionalCat_id'].str.split(',', expand = True)\n",
    "dummies = pd.get_dummies(mrs[['func1', 'func2', 'func3', 'func4', 'func5', 'func6', 'func7']])\n",
    "dummies = dummies.groupby(dummies.columns.str.split(\"_\").str[1],axis=1).sum()\n",
    "dummies['total'] = dummies.sum(axis=1)\n",
    "dummies = dummies.rename(columns = lambda x : 'func_' + x)\n",
    "mrs = mrs.drop(['functionalCat_id', 'functionalCategory','func1', 'func2', 'func3', 'func4', 'func5', 'func6', 'func7'], axis = 1)\n",
    "mrs = pd.concat([mrs, dummies], axis=1, join='inner')\n",
    "mrs.columns\n",
    "\n",
    "#techCatName\n",
    "#techCatId\n",
    "mrs[['tech1', 'tech2', 'tech3', 'tech4', 'tech5', 'tech6', 'tech7', 'tech8']] = \\\n",
    "mrs['technicalCat_id'].str.split(',', expand = True)\n",
    "dummies = pd.get_dummies(mrs[['tech1', 'tech2', 'tech3', 'tech4', 'tech5', 'tech6', 'tech7', 'tech8']])\n",
    "dummies = dummies.groupby(dummies.columns.str.split(\"_\").str[1],axis=1).sum()\n",
    "dummies['total'] = dummies.sum(axis=1)\n",
    "dummies = dummies.rename(columns = lambda x : 'tech_' + x)\n",
    "mrs = mrs.drop(['technicalCat_id', 'technicalCategory','tech1', 'tech2', 'tech3', 'tech4', 'tech5', 'tech6', 'tech7', 'tech8'], axis = 1)\n",
    "mrs = pd.concat([mrs, dummies], axis=1, join='inner')\n",
    "mrs.columns\n",
    "\n",
    "\n",
    "#mainCatName\n",
    "#mainCatId\n",
    "\n",
    "mrs[['main1', 'main2', 'main3', 'main4']] = mrs['mainCat_id'].str.split(',', expand = True)\n",
    "dummies = pd.get_dummies(mrs[['main1', 'main2', 'main3', 'main4']])\n",
    "dummies = dummies.groupby(dummies.columns.str.split(\"_\").str[1],axis=1).sum()\n",
    "dummies['total'] = dummies.sum(axis=1)\n",
    "dummies = dummies.rename(columns = lambda x : 'main_' + x)\n",
    "mrs = mrs.drop(['mainCat_id', 'mainCategory','main1', 'main2', 'main3', 'main4'], axis = 1)\n",
    "mrs = pd.concat([mrs, dummies], axis=1, join='inner')\n",
    "\n",
    "\n",
    "#categorys = ['functionalCategory', 'functionalCat_id', 'technicalCategory', 'technicalCat_id',\n",
    "#       'mainCategory', 'mainCat_id' ]\n",
    "#for col in categorys:\n",
    "#    if(col in mrs.columns):\n",
    "#        mrs = mrs.drop(col,1)\n",
    "#These all formatted as such value, [another value, another value]\n",
    "#For both Ids and Name. Have to decide how we're going to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postProcessDrop = ['i9', 'year_began', 'ivy', 'minority', 'gender', 'email',\n",
    "                   'clarity', 'personability', 'status_name']\n",
    "\n",
    "\n",
    "strings = ['status_name', 'title', 'date_submitted', 'fname', 'lname', 'sourceName', 'gender', 'date_created', 'i9',\n",
    "          'email', 'address', 'minority', 'ivy','address_field','clarity','personability','zip']\n",
    "\n",
    "for col in postProcessDrop:\n",
    "    if(col in mrs.columns):\n",
    "        mrs = mrs.drop(col,1)\n",
    "        \n",
    "for col in strings:\n",
    "    if(col in mrs.columns):\n",
    "        mrs = mrs.drop(col,1)\n",
    "allCol=mrs.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = mrs.groupby(mrs.columns.str.split(\"_\").str[1],axis=1).sum()\n",
    "temp = temp.rename(columns = lambda x : 'cat_' + x)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrs['yearsOfExp'].fillna(value=0, inplace=True)\n",
    "Possible_values = {col: mrs[col].isnull().sum() for col in mrs.columns}\n",
    "Possible_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test data split\n",
    "train_split = 0.7\n",
    "train = mrs.sample(frac=train_split)\n",
    "print (\"Train dataset info:\\n\", train.is_placed.value_counts())\n",
    "test = mrs.loc[~mrs.index.isin(train.index)]\n",
    "print (\"Test dataset info:\\n\", test.is_placed.value_counts())\n",
    "\n",
    "train.to_csv(\"cleaned_mostRecStat_train.csv\",index=False)\n",
    "test.to_csv(\"cleaned_mostRecStat_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
